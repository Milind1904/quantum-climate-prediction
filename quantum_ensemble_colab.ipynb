{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe7bf37",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c293ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qiskit qiskit-machine-learning qiskit-aer scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a092e5",
   "metadata": {},
   "source": [
    "## Step 2: Upload Your Data\n",
    "\n",
    "**Option A**: Upload the CSV files from `data/extracted_data/extracted_data/` folder\n",
    "\n",
    "**Option B**: If you have the preprocessed data, upload `X_scaled.npy` and `y.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload CSV files (or skip if using preprocessed data)\n",
    "print(\"Upload your climate CSV files...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs('data/extracted_data/extracted_data', exist_ok=True)\n",
    "\n",
    "# Move uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.csv'):\n",
    "        !mv {filename} data/extracted_data/extracted_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a3d4f",
   "metadata": {},
   "source": [
    "## Step 3: Define Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def load_and_prepare_climate_data():\n",
    "    \"\"\"\n",
    "    Load and prepare climate data from CSV files\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LOADING CLIMATE DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_files = glob.glob('data/extracted_data/extracted_data/*.csv')\n",
    "    print(f\"\\n[INFO] Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        raise FileNotFoundError(\"No CSV files found. Please upload your data.\")\n",
    "    \n",
    "    # Load all CSV files\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nâœ“ Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"âœ“ Total samples: {len(combined_df)}\")\n",
    "    \n",
    "    # Extract features (exclude metadata columns)\n",
    "    metadata_cols = ['filename', 'latitude', 'longitude', 'altitude', 'time_index', 'time']\n",
    "    feature_cols = [col for col in combined_df.columns if col not in metadata_cols]\n",
    "    \n",
    "    X = combined_df[feature_cols].copy()\n",
    "    print(f\"\\n[INFO] Features extracted: {X.shape}\")\n",
    "    \n",
    "    # Create labels based on DBZ_max > 30 dBZ (extreme weather threshold)\n",
    "    if 'DBZ_max' in combined_df.columns:\n",
    "        y = (combined_df['DBZ_max'] > 30).astype(int).values\n",
    "        print(f\"\\n[INFO] Labels created based on DBZ_max > 30 dBZ\")\n",
    "    else:\n",
    "        raise ValueError(\"DBZ_max column not found in dataset\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_count = X.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"\\nâš ï¸  Found {missing_count} missing values, filling with column means...\")\n",
    "        X = X.fillna(X.mean())\n",
    "        print(\"âœ“ Missing values handled\")\n",
    "    \n",
    "    # Display class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\n[INFO] Class distribution:\")\n",
    "    print(f\"  Normal weather (0): {counts[0]} samples ({counts[0]/len(y)*100:.1f}%)\")\n",
    "    print(f\"  Extreme weather (1): {counts[1]} samples ({counts[1]/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final dataset ready:\")\n",
    "    print(f\"  X shape: {X.shape}\")\n",
    "    print(f\"  y shape: {y.shape}\")\n",
    "    \n",
    "    return X.values, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4eb79a",
   "metadata": {},
   "source": [
    "## Step 4: Quantum Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "from qiskit.circuit.library import ZZFeatureMap, PauliFeatureMap\n",
    "import time\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"QUANTUM ENSEMBLE MODEL - COLAB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load data\n",
    "print(\"\\n[1/7] Loading climate data...\")\n",
    "X, y = load_and_prepare_climate_data()\n",
    "print(f\"âœ“ Loaded {len(X)} samples with {X.shape[1]} features\")\n",
    "\n",
    "# Normalize features\n",
    "print(\"\\n[2/7] Normalizing features...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Prepare training data (1000 samples for good balance of speed and accuracy)\n",
    "print(\"\\n[3/7] Preparing training data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Sample data with natural distribution\n",
    "class_0_indices = np.where(y_train == 0)[0]\n",
    "class_1_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# 1000 training samples (85% class 0, 15% class 1)\n",
    "n_train_samples = 1000\n",
    "n_class_0 = int(n_train_samples * 0.85)\n",
    "n_class_1 = n_train_samples - n_class_0\n",
    "\n",
    "selected_0 = np.random.choice(class_0_indices, size=n_class_0, replace=False)\n",
    "selected_1 = np.random.choice(class_1_indices, size=n_class_1, replace=False)\n",
    "train_indices = np.concatenate([selected_0, selected_1])\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "X_train_sampled = X_train[train_indices]\n",
    "y_train_sampled = y_train[train_indices]\n",
    "\n",
    "# 500 test samples\n",
    "test_class_0 = np.where(y_test == 0)[0]\n",
    "test_class_1 = np.where(y_test == 1)[0]\n",
    "n_test_samples = 500\n",
    "n_test_0 = int(n_test_samples * 0.85)\n",
    "n_test_1 = n_test_samples - n_test_0\n",
    "\n",
    "selected_test_0 = np.random.choice(test_class_0, size=n_test_0, replace=False)\n",
    "selected_test_1 = np.random.choice(test_class_1, size=n_test_1, replace=False)\n",
    "test_indices = np.concatenate([selected_test_0, selected_test_1])\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "X_test_sampled = X_test[test_indices]\n",
    "y_test_sampled = y_test[test_indices]\n",
    "\n",
    "print(f\"âœ“ Training samples: {len(X_train_sampled)} (Class 0: {n_class_0}, Class 1: {n_class_1})\")\n",
    "print(f\"âœ“ Test samples: {len(X_test_sampled)}\")\n",
    "\n",
    "# Create ensemble of 3 quantum models\n",
    "print(\"\\n[4/7] Building Quantum Ensemble (3 models)...\")\n",
    "print(\"â±ï¸  Estimated time: 15-25 minutes\\n\")\n",
    "\n",
    "ensemble_predictions = []\n",
    "individual_accuracies = []\n",
    "individual_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90b602",
   "metadata": {},
   "source": [
    "### Model 1: 4-qubit ZZ FeatureMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 1/3: 4-qubit ZZ FeatureMap...\")\n",
    "start_time_1 = time.time()\n",
    "\n",
    "pca_4 = PCA(n_components=4)\n",
    "X_train_pca4 = pca_4.fit_transform(X_train_sampled)\n",
    "X_test_pca4 = pca_4.transform(X_test_sampled)\n",
    "\n",
    "feature_map_4 = ZZFeatureMap(feature_dimension=4, reps=2, entanglement='linear')\n",
    "quantum_kernel_4 = FidelityQuantumKernel(feature_map=feature_map_4)\n",
    "\n",
    "print(\"  Computing quantum kernel matrices...\")\n",
    "K_train_4 = quantum_kernel_4.evaluate(X_train_pca4)\n",
    "K_test_4 = quantum_kernel_4.evaluate(X_test_pca4, X_train_pca4)\n",
    "\n",
    "qsvc_4 = SVC(kernel='precomputed', C=15.0, class_weight='balanced')\n",
    "qsvc_4.fit(K_train_4, y_train_sampled)\n",
    "y_pred_4 = qsvc_4.predict(K_test_4)\n",
    "ensemble_predictions.append(y_pred_4)\n",
    "\n",
    "acc_4 = accuracy_score(y_test_sampled, y_pred_4)\n",
    "time_4 = time.time() - start_time_1\n",
    "individual_accuracies.append(acc_4)\n",
    "individual_times.append(time_4)\n",
    "print(f\"âœ“ Model 1: {acc_4:.4f} ({acc_4*100:.2f}%) in {time_4:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4eb7c1",
   "metadata": {},
   "source": [
    "### Model 2: 5-qubit ZZ FeatureMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe305b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 2/3: 5-qubit ZZ FeatureMap...\")\n",
    "start_time_2 = time.time()\n",
    "\n",
    "pca_5 = PCA(n_components=5)\n",
    "X_train_pca5 = pca_5.fit_transform(X_train_sampled)\n",
    "X_test_pca5 = pca_5.transform(X_test_sampled)\n",
    "\n",
    "feature_map_5 = ZZFeatureMap(feature_dimension=5, reps=2, entanglement='full')\n",
    "quantum_kernel_5 = FidelityQuantumKernel(feature_map=feature_map_5)\n",
    "\n",
    "print(\"  Computing quantum kernel matrices...\")\n",
    "K_train_5 = quantum_kernel_5.evaluate(X_train_pca5)\n",
    "K_test_5 = quantum_kernel_5.evaluate(X_test_pca5, X_train_pca5)\n",
    "\n",
    "qsvc_5 = SVC(kernel='precomputed', C=12.0, class_weight='balanced')\n",
    "qsvc_5.fit(K_train_5, y_train_sampled)\n",
    "y_pred_5 = qsvc_5.predict(K_test_5)\n",
    "ensemble_predictions.append(y_pred_5)\n",
    "\n",
    "acc_5 = accuracy_score(y_test_sampled, y_pred_5)\n",
    "time_5 = time.time() - start_time_2\n",
    "individual_accuracies.append(acc_5)\n",
    "individual_times.append(time_5)\n",
    "print(f\"âœ“ Model 2: {acc_5:.4f} ({acc_5*100:.2f}%) in {time_5:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6683d59",
   "metadata": {},
   "source": [
    "### Model 3: 6-qubit Pauli FeatureMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 3/3: 6-qubit Pauli FeatureMap...\")\n",
    "start_time_3 = time.time()\n",
    "\n",
    "pca_6 = PCA(n_components=6)\n",
    "X_train_pca6 = pca_6.fit_transform(X_train_sampled)\n",
    "X_test_pca6 = pca_6.transform(X_test_sampled)\n",
    "\n",
    "feature_map_6 = PauliFeatureMap(feature_dimension=6, reps=2, paulis=['Z', 'ZZ'])\n",
    "quantum_kernel_6 = FidelityQuantumKernel(feature_map=feature_map_6)\n",
    "\n",
    "print(\"  Computing quantum kernel matrices...\")\n",
    "K_train_6 = quantum_kernel_6.evaluate(X_train_pca6)\n",
    "K_test_6 = quantum_kernel_6.evaluate(X_test_pca6, X_train_pca6)\n",
    "\n",
    "qsvc_6 = SVC(kernel='precomputed', C=10.0, class_weight='balanced')\n",
    "qsvc_6.fit(K_train_6, y_train_sampled)\n",
    "y_pred_6 = qsvc_6.predict(K_test_6)\n",
    "ensemble_predictions.append(y_pred_6)\n",
    "\n",
    "acc_6 = accuracy_score(y_test_sampled, y_pred_6)\n",
    "time_6 = time.time() - start_time_3\n",
    "individual_accuracies.append(acc_6)\n",
    "individual_times.append(time_6)\n",
    "print(f\"âœ“ Model 3: {acc_6:.4f} ({acc_6*100:.2f}%) in {time_6:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6a3d2",
   "metadata": {},
   "source": [
    "## Step 5: Ensemble Voting and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble voting (majority vote)\n",
    "print(\"[5/7] Combining predictions with majority voting...\")\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "y_pred_ensemble = np.apply_along_axis(\n",
    "    lambda x: np.bincount(x.astype(int)).argmax(), \n",
    "    axis=0, \n",
    "    arr=ensemble_predictions\n",
    ")\n",
    "\n",
    "# Calculate ensemble metrics\n",
    "print(\"\\n[6/7] Calculating ensemble metrics...\")\n",
    "accuracy = accuracy_score(y_test_sampled, y_pred_ensemble)\n",
    "precision = precision_score(y_test_sampled, y_pred_ensemble, zero_division=0)\n",
    "recall = recall_score(y_test_sampled, y_pred_ensemble, zero_division=0)\n",
    "f1 = f1_score(y_test_sampled, y_pred_ensemble, zero_division=0)\n",
    "cm = confusion_matrix(y_test_sampled, y_pred_ensemble)\n",
    "\n",
    "total_time = sum(individual_times)\n",
    "\n",
    "print(\"\\n[7/7] ENSEMBLE RESULTS:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Individual Model Accuracies:\")\n",
    "print(f\"  Model 1 (4-qubit ZZ):    {acc_4:.4f} ({acc_4*100:.2f}%)\")\n",
    "print(f\"  Model 2 (5-qubit ZZ):    {acc_5:.4f} ({acc_5*100:.2f}%)\")\n",
    "print(f\"  Model 3 (6-qubit Pauli): {acc_6:.4f} ({acc_6*100:.2f}%)\")\n",
    "print(f\"\\nðŸŽ¯ ENSEMBLE PERFORMANCE (Majority Voting):\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTotal Training Time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Compare with original QSVC\n",
    "print(f\"\\nðŸ“Š Comparison:\")\n",
    "print(f\"  Original QSVC (1000 samples): 78.6%\")\n",
    "print(f\"  Ensemble (1000 samples):      {accuracy*100:.2f}%\")\n",
    "if accuracy > 0.786:\n",
    "    improvement = (accuracy - 0.786) * 100\n",
    "    print(f\"  âœ… Ensemble improved by {improvement:.2f} percentage points!\")\n",
    "elif accuracy >= 0.78:\n",
    "    print(f\"  âœ“ Similar performance - ensemble confirms quantum ML viability\")\n",
    "else:\n",
    "    print(f\"  â†’ Original QSVC remains slightly better\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"model_type\": \"Quantum_Ensemble_Colab\",\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1_score\": float(f1),\n",
    "    \"individual_accuracies\": {\n",
    "        \"model_1_4qubit\": float(acc_4),\n",
    "        \"model_2_5qubit\": float(acc_5),\n",
    "        \"model_3_6qubit\": float(acc_6)\n",
    "    },\n",
    "    \"individual_times\": {\n",
    "        \"model_1\": float(time_4),\n",
    "        \"model_2\": float(time_5),\n",
    "        \"model_3\": float(time_6)\n",
    "    },\n",
    "    \"total_time\": float(total_time),\n",
    "    \"training_samples\": int(len(X_train_sampled)),\n",
    "    \"test_samples\": int(len(X_test_sampled)),\n",
    "    \"ensemble_method\": \"majority_voting\",\n",
    "    \"num_models\": 3\n",
    "}\n",
    "\n",
    "with open('quantum_ensemble_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ“ Results saved to quantum_ensemble_results.json\")\n",
    "print(\"\\nâœ… ENSEMBLE TRAINING COMPLETE!\")\n",
    "\n",
    "# Download results\n",
    "files.download('quantum_ensemble_results.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
